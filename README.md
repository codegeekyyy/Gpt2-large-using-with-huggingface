# Gpt2-large-using-with-huggingface
This project demonstrates text generation with GPT-2 Large using the Hugging Face Transformers library. It explores various decoding strategies such as Greedy Search, Beam Search, Top-k Sampling, and Nucleus (Top-p) Sampling, showcasing how each impacts fluency, coherence, and creativity in AI-generated text.
